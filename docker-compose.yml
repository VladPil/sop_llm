services:
  # Redis for caching and FastStream queue
  redis:
    image: redis:7-alpine
    container_name: sop_llm_redis
    command: redis-server /usr/local/etc/redis/redis.conf
    ports:
      - "${REDIS_PORT:-6381}:6379"
    volumes:
      - sop_llm_redis_data:/data
      - ./.docker/redis/redis.conf:/usr/local/etc/redis/redis.conf:ro
      - sop_llm_redis_logs:/var/log/redis
    networks:
      - sop_llm_network
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
    restart: unless-stopped

  # LLM Application
  app:
    build:
      context: .
      dockerfile: .docker/app/Dockerfile
      args:
        - PYTHON_VERSION=${PYTHON_VERSION:-3.11}
        - INSTALL_GPU=${INSTALL_GPU:-false}
        - CUDA_VERSION=${CUDA_VERSION:-12.1.0}
    image: sop_llm:${APP_VERSION:-latest}
    container_name: sop_llm_app
    depends_on:
      redis:
        condition: service_healthy
    ports:
      - "${APP_PORT:-8001}:${API_PORT:-8000}"
      - "${METRICS_PORT:-9091}:9090"
    volumes:
      # Монтируем весь проект в /app
      - .:/app
      # Кэш моделей в отдельном volume для персистентности
      - sop_llm_models_cache:/root/.cache/huggingface
      # Логи в отдельном volume
      - sop_llm_app_logs:/app/logs
    networks:
      - sop_llm_network
    environment:
      # Application
      - APP_NAME=${APP_NAME:-sop_llm}
      - APP_ENV=${APP_ENV:-production}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - LOG_FORMAT=${LOG_FORMAT:-json}
      - API_PORT=${API_PORT}

      # Redis
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD:-}
      - REDIS_DB=${REDIS_DB:-0}

      # HuggingFace
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN:-}
      - HF_HOME=/root/.cache/huggingface

      # LLM Settings
      - MODEL_NAME=${MODEL_NAME:-sentence-transformers/all-MiniLM-L6-v2}
      - DEVICE=${DEVICE:-cuda}
      - MAX_BATCH_SIZE=${MAX_BATCH_SIZE:-32}
      - MAX_SEQ_LENGTH=${MAX_SEQ_LENGTH:-512}

      # API Keys (if using external APIs)
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}

      # Performance
      - WORKERS=${WORKERS:-1}
      - THREADS=${THREADS:-4}
      - TORCH_NUM_THREADS=${TORCH_NUM_THREADS:-4}
      - OMP_NUM_THREADS=${OMP_NUM_THREADS:-4}
      # GPU support
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    healthcheck:
      test: ["CMD", "/app/scripts/healthcheck.sh"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    stop_grace_period: 10s
    # Enable GPU support
    runtime: nvidia

  # Optional: Redis Commander for Redis management UI
  redis-commander:
    image: rediscommander/redis-commander:latest
    container_name: sop_llm_redis_commander
    depends_on:
      redis:
        condition: service_healthy
    ports:
      - "${REDIS_COMMANDER_PORT:-8082}:8081"
    environment:
      - REDIS_HOSTS=local:redis:6379:0:${REDIS_PASSWORD:-sop_llm_redis_password}
    networks:
      - sop_llm_network
    profiles:
      - dev
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
    restart: unless-stopped

networks:
  sop_llm_network:
    driver: bridge
    name: sop_llm_network

volumes:
  sop_llm_redis_data:
    name: sop_llm_redis_data
  sop_llm_redis_logs:
    name: sop_llm_redis_logs
  sop_llm_models_cache:
    name: sop_llm_models_cache
  sop_llm_app_logs:
    name: sop_llm_app_logs
