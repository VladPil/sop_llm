# Пресеты локальных GGUF моделей (прямая загрузка через llama-cpp-python)
#
# ВАЖНО: Для локальных GPU моделей рекомендуется использовать Ollama.
# Ollama модели находятся в cloud_models.yaml (секция Ollama).
#
# Этот файл используется только для прямой работы с GGUF файлами
# без Ollama, например для специфичных квантизаций или кастомных моделей.
#
# Для использования Ollama моделей:
# 1. Запустить: make up-gpu
# 2. Модели доступны как: qwen2.5:7b, llama3.1:8b, mistral:7b и др.

models: []
# Примеры прямых GGUF моделей (раскомментировать при необходимости):
#
#  - name: "custom-qwen-q8"
#    huggingface_repo: "Qwen/Qwen2.5-7B-Instruct-GGUF"
#    filename: "qwen2.5-7b-instruct-q8_0.gguf"
#    size_b: 7
#    vram_requirements:
#      default: 9000
#    provider_config:
#      context_window: 32768
#      gpu_layers: -1
