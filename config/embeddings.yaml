# ==============================================
# Embedding Models Configuration
# ==============================================
# This file defines all available embedding models for text vectorization

embedding_models:
  # --------------------------------------------
  # E5-base-v2 - Multilingual text embeddings
  # --------------------------------------------
  - name: e5-base
    display_name: "E5 Base v2"
    hf_repo: "intfloat/e5-base-v2"
    model_type: "sentence_transformer"

    # Model specifications
    embedding_dimension: 768
    max_seq_length: 512
    languages:
      - en
      - zh
      - ru
      - es
      - fr
      - de

    # Resource settings
    device: cuda  # cuda, cpu, or auto
    max_concurrent: 8  # Can handle many concurrent requests
    memory_limit: 2  # GB

    # Batch processing
    batch_size: 32  # Number of texts to process at once
    max_batch_wait: 0.1  # Max seconds to wait for batch to fill

    # Processing options
    normalize_embeddings: true  # L2 normalization
    convert_to_numpy: false  # Return as numpy arrays (vs tensors)
    show_progress_bar: false  # Disable in production

    # Performance tuning
    use_fp16: true  # Use float16 for faster inference (GPU only)
    compile_model: false  # PyTorch 2.0 compilation (experimental)

    # Timeouts and limits
    idle_timeout: 300  # Unload after 5 minutes of inactivity
    max_texts_per_request: 100

    # Special tokens/instructions (E5 specific)
    query_instruction_prefix: "query: "
    passage_instruction_prefix: "passage: "

  # --------------------------------------------
  # E5-large-v2 - Higher quality embeddings
  # --------------------------------------------
  - name: e5-large
    display_name: "E5 Large v2"
    hf_repo: "intfloat/e5-large-v2"
    model_type: "sentence_transformer"

    # Model specifications
    embedding_dimension: 1024
    max_seq_length: 512
    languages:
      - en
      - zh
      - ru
      - es
      - fr
      - de

    # Resource settings
    device: cuda
    max_concurrent: 4
    memory_limit: 4

    # Batch processing
    batch_size: 16
    max_batch_wait: 0.1

    # Processing options
    normalize_embeddings: true
    convert_to_numpy: false
    show_progress_bar: false

    # Performance tuning
    use_fp16: true
    compile_model: false

    # Timeouts and limits
    idle_timeout: 300
    max_texts_per_request: 100

    # Special tokens/instructions
    query_instruction_prefix: "query: "
    passage_instruction_prefix: "passage: "

  # --------------------------------------------
  # BERT-base - Lightweight embeddings
  # --------------------------------------------
  - name: bert-base
    display_name: "BERT Base (MiniLM)"
    hf_repo: "sentence-transformers/all-MiniLM-L6-v2"
    model_type: "sentence_transformer"

    # Model specifications
    embedding_dimension: 384
    max_seq_length: 256
    languages:
      - en  # English only

    # Resource settings
    device: cpu  # Works well on CPU
    max_concurrent: 8
    memory_limit: 1.5

    # Batch processing
    batch_size: 32
    max_batch_wait: 0.05

    # Processing options
    normalize_embeddings: true
    convert_to_numpy: false
    show_progress_bar: false

    # Performance tuning
    use_fp16: false  # Not recommended for CPU
    compile_model: false

    # Timeouts and limits
    idle_timeout: 600  # Keep longer (cheap to run on CPU)
    max_texts_per_request: 200

  # --------------------------------------------
  # BERT-large - High quality English embeddings
  # --------------------------------------------
  - name: bert-large
    display_name: "BERT Large (MPNet)"
    hf_repo: "sentence-transformers/all-mpnet-base-v2"
    model_type: "sentence_transformer"

    # Model specifications
    embedding_dimension: 768
    max_seq_length: 384
    languages:
      - en  # English only

    # Resource settings
    device: cuda
    max_concurrent: 4
    memory_limit: 3

    # Batch processing
    batch_size: 16
    max_batch_wait: 0.1

    # Processing options
    normalize_embeddings: true
    convert_to_numpy: false
    show_progress_bar: false

    # Performance tuning
    use_fp16: true
    compile_model: false

    # Timeouts and limits
    idle_timeout: 300
    max_texts_per_request: 100

  # --------------------------------------------
  # Multilingual E5 - Support for 100+ languages
  # --------------------------------------------
  - name: e5-multilingual
    display_name: "E5 Multilingual Base"
    hf_repo: "intfloat/multilingual-e5-base"
    model_type: "sentence_transformer"

    # Model specifications
    embedding_dimension: 768
    max_seq_length: 512
    languages:
      - multilingual  # 100+ languages

    # Resource settings
    device: cuda
    max_concurrent: 6
    memory_limit: 2.5

    # Batch processing
    batch_size: 24
    max_batch_wait: 0.1

    # Processing options
    normalize_embeddings: true
    convert_to_numpy: false
    show_progress_bar: false

    # Performance tuning
    use_fp16: true
    compile_model: false

    # Timeouts and limits
    idle_timeout: 300
    max_texts_per_request: 100

    # Special tokens/instructions
    query_instruction_prefix: "query: "
    passage_instruction_prefix: "passage: "

  # --------------------------------------------
  # BGE-base - Chinese/English specialized
  # --------------------------------------------
  - name: bge-base
    display_name: "BGE Base (Chinese/English)"
    hf_repo: "BAAI/bge-base-en-v1.5"
    model_type: "sentence_transformer"

    # Model specifications
    embedding_dimension: 768
    max_seq_length: 512
    languages:
      - en
      - zh

    # Resource settings
    device: cuda
    max_concurrent: 8
    memory_limit: 2

    # Batch processing
    batch_size: 32
    max_batch_wait: 0.1

    # Processing options
    normalize_embeddings: true
    convert_to_numpy: false
    show_progress_bar: false

    # Performance tuning
    use_fp16: true
    compile_model: false

    # Timeouts and limits
    idle_timeout: 300
    max_texts_per_request: 100

# ==============================================
# Global Embedding Settings
# ==============================================
global_settings:
  # Default settings applied to all models
  default_batch_size: 32
  default_normalize: true
  default_max_length: 512

  # Resource management
  max_total_memory: 8  # GB - Total memory limit across all embedding models
  enable_model_sharing: false  # Share models between requests

  # Caching
  cache_embeddings: true  # Cache computed embeddings
  cache_max_entries: 10000  # Maximum cached embeddings
  cache_strategy: "lru"  # lru, fifo, or lfu

  # Auto-batching
  enable_auto_batching: true  # Automatically batch multiple requests
  auto_batch_timeout: 0.05  # Seconds to wait for batch to fill
  auto_batch_max_size: 64  # Maximum auto-batch size

  # Pooling strategies
  default_pooling: "mean"  # mean, cls, max

  # Monitoring
  log_embedding_stats: true  # Log processing time, batch sizes
  enable_profiling: false

# ==============================================
# Model Preloading
# ==============================================
# Models to load on startup (leave empty to lazy load)
preload_models:
  - bert-base  # Fast CPU model for immediate availability
  - e5-base    # General purpose GPU model

# ==============================================
# Task-specific Configurations
# ==============================================
# Optimized settings for different use cases
task_profiles:
  # Semantic search / retrieval
  semantic_search:
    recommended_models:
      - e5-large
      - e5-base
      - bge-base
    normalize: true
    pooling: "mean"

  # Text similarity / clustering
  similarity:
    recommended_models:
      - bert-large
      - e5-base
    normalize: true
    pooling: "mean"

  # Classification / downstream tasks
  classification:
    recommended_models:
      - bert-large
      - e5-large
    normalize: false
    pooling: "cls"

  # Cross-lingual / multilingual
  multilingual:
    recommended_models:
      - e5-multilingual
      - e5-base
    normalize: true
    pooling: "mean"

  # Fast/lightweight (high throughput)
  lightweight:
    recommended_models:
      - bert-base
    normalize: true
    pooling: "mean"
    use_fp16: true
    batch_size: 64

# ==============================================
# Hardware-specific Configurations
# ==============================================
hardware_profiles:
  # High-end GPU
  high_end_gpu:
    device: cuda
    use_fp16: true
    batch_size: 64
    max_concurrent: 16

  # Mid-range GPU
  mid_range_gpu:
    device: cuda
    use_fp16: true
    batch_size: 32
    max_concurrent: 8

  # CPU only
  cpu_only:
    device: cpu
    use_fp16: false
    batch_size: 16
    max_concurrent: 4

# Active profile (comment out to auto-detect)
# active_profile: mid_range_gpu
