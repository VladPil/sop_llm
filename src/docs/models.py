"""Documentation strings для Models API."""

LIST_MODELS = """
Возвращает полный список всех моделей, зарегистрированных в системе.

## Что возвращает

Для каждой модели возвращается:
- **name** — уникальное имя модели в системе (используется в запросах генерации)
- **provider** — тип провайдера (`openai`, `anthropic`, `local`, `openai_compatible`)
- **context_window** — максимальный размер контекста в токенах
- **max_output_tokens** — максимальное количество токенов в ответе
- **supports_streaming** — поддерживает ли модель потоковую генерацию
- **supports_structured_output** — поддерживает ли JSON Schema / GBNF грамматики
- **loaded** — загружена ли модель в память (для локальных моделей)

## Типы провайдеров

| Провайдер | Описание |
|-----------|----------|
| `openai` | OpenAI API (GPT-4, GPT-3.5) |
| `anthropic` | Anthropic API (Claude) |
| `local` | Локальные GGUF модели через llama.cpp |
| `openai_compatible` | Любой OpenAI-совместимый API (Ollama, vLLM) |

## Примечания

- Модели регистрируются при старте приложения или через API `/models/register`
- Для добавления новых моделей используйте `/models/register` или `/models/register-from-preset`
"""

GET_MODEL_INFO = """
Возвращает подробную информацию о конкретной зарегистрированной модели.

## Параметры

- **model_name** (path) — уникальное имя модели в системе

## Что возвращает

- **name** — имя модели
- **provider** — тип провайдера
- **context_window** — размер контекстного окна (в токенах)
- **max_output_tokens** — максимум токенов в ответе
- **supports_streaming** — поддержка потоковой генерации
- **supports_structured_output** — поддержка JSON Schema / GBNF
- **loaded** — статус загрузки (для локальных моделей)
- **extra** — дополнительные метаданные (VRAM usage, версия API и т.д.)

## Ошибки

- **404 Not Found** — модель с указанным именем не зарегистрирована
"""

REGISTER_MODEL = """
Динамически регистрирует новую модель в системе с полной конфигурацией.

## Когда использовать

Используйте этот endpoint, когда нужно:
- Подключить модель с нестандартными параметрами
- Зарегистрировать модель, отсутствующую в пресетах
- Подключить кастомный OpenAI-совместимый API

**Для стандартных моделей рекомендуется использовать** `/models/register-from-preset`

## Типы провайдеров и конфигурация

| Провайдер | Обязательные поля в config |
|-----------|---------------------------|
| `openai` | `api_key`, `model_name` |
| `anthropic` | `api_key`, `model_name` |
| `local` | `model_path`, `context_window` |
| `openai_compatible` | `model_name`, `base_url`, `api_key` |

### Дополнительные поля config

| Поле | Тип | Описание |
|------|-----|----------|
| `timeout` | int | Таймаут запроса в секундах (default: 120) |
| `max_retries` | int | Количество повторов при ошибке (default: 3) |
| `gpu_layers` | int | Слои на GPU для local (-1 = все) |

## Ошибки

- **400 Bad Request** — невалидная конфигурация или ошибка инициализации провайдера
- **409 Conflict** — модель с таким именем уже зарегистрирована
"""

UNREGISTER_MODEL = """
Удаляет зарегистрированную модель из системы.

## Что происходит при удалении

1. Модель удаляется из registry и становится недоступной для генерации
2. Если `cleanup=true` (по умолчанию):
   - Для локальных моделей: выгружается из GPU/RAM
   - Для облачных: закрываются HTTP соединения

## Параметры

| Параметр | Тип | Описание |
|----------|-----|----------|
| `model_name` | path | Имя модели для удаления |
| `cleanup` | query | Выполнить очистку ресурсов (default: true) |

## Важно

- Файлы моделей НЕ удаляются с диска
- Нельзя удалить модель во время выполнения задачи
- После удаления модель можно зарегистрировать заново

## Ошибки

- **404 Not Found** — модель не найдена в registry
"""

LIST_PRESETS = """
Возвращает список всех предустановленных конфигураций моделей из YAML файлов.

## Что такое пресеты

Пресеты — это готовые конфигурации моделей, которые можно быстро зарегистрировать
через `/models/register-from-preset` без ручного указания всех параметров.

## Типы пресетов

### Локальные модели (local_models)
GGUF модели для запуска на GPU через llama.cpp:
- Qwen 2.5 (3B, 7B, 14B, Coder)
- LLaMA 3.2 (3B, 8B)
- Mistral 7B, Phi-3, Gemma 2

Содержат информацию о:
- HuggingFace репозитории для автозагрузки
- Требованиях VRAM для разных квантизаций
- Размере контекстного окна

### Облачные модели (cloud_models)
API провайдеры (требуют API ключи):
- Claude 3.5 Sonnet/Haiku, Claude 3 Opus
- GPT-4 Turbo, GPT-4o, GPT-4o-mini
- Gemini Pro, Mistral Large
- Groq, Together AI, DeepSeek

### Embedding модели (embedding_models)
Модели для векторизации текста:
- E5 (multilingual-large, base, small)
- Sentence Transformers
- BGE, Cohere, Jina

## Следующие шаги

После получения списка пресетов:
1. Выберите нужный пресет
2. Проверьте совместимость с GPU: `POST /models/check-compatibility`
3. Зарегистрируйте: `POST /models/register-from-preset`
"""

REGISTER_FROM_PRESET = """
Регистрирует модель из предустановленного пресета с автоматической загрузкой.

## Как это работает

### Для локальных моделей (GGUF)
1. Проверяется совместимость с GPU (доступная VRAM)
2. Если модель отсутствует локально и `auto_download=true` — скачивается с HuggingFace
3. Модель загружается в GPU/RAM
4. Регистрируется в системе

### Для облачных моделей
1. Проверяется наличие API ключа в переменных окружения
2. Создаётся подключение к API провайдера
3. Модель регистрируется в системе

## Параметры запроса

| Параметр | Тип | Описание |
|----------|-----|----------|
| `preset_name` | string | Имя пресета из `/models/presets` |
| `auto_download` | bool | Автозагрузка с HuggingFace (default: true) |
| `quantization` | string | Переопределить квантизацию (q4_k_m, q5_k_m, q8_0, fp16) |

## Рекомендации

- Перед регистрацией локальной модели проверьте совместимость через `/models/check-compatibility`
- Для облачных моделей убедитесь что API ключ установлен в переменных окружения

## Ошибки

- **400** — ошибка загрузки или инициализации модели
- **404** — пресет не найден
- **409** — модель с таким именем уже зарегистрирована
"""

CHECK_COMPATIBILITY = """
Проверяет, поместится ли локальная модель в доступную видеопамять (VRAM).

## Зачем это нужно

Локальные GGUF модели требуют значительного объёма VRAM для работы.
Этот endpoint позволяет заранее проверить:
- Поместится ли модель в доступную VRAM
- Какую квантизацию лучше использовать
- Сколько памяти потребуется

## Как работает

1. Получает требования VRAM из пресета модели
2. Запрашивает текущую доступную VRAM у GPU
3. Сравнивает и выдаёт рекомендацию

## Параметры запроса

| Параметр | Тип | Описание |
|----------|-----|----------|
| `preset_name` | string | Имя локального пресета |
| `quantization` | string | Квантизация для проверки (опционально) |

## Квантизации

| Квантизация | Размер | Качество | Пример для 7B модели |
|-------------|--------|----------|---------------------|
| `q4_k_m` | ~50% | Хорошее | ~4-5 GB |
| `q5_k_m` | ~60% | Очень хорошее | ~5-6 GB |
| `q8_0` | ~90% | Отличное | ~8-9 GB |
| `fp16` | 100% | Максимальное | ~14 GB |

## Что возвращает

| Поле | Описание |
|------|----------|
| `compatible` | Поместится ли модель в VRAM |
| `required_vram_mb` | Требуемая VRAM в MB |
| `available_vram_mb` | Доступная VRAM в MB |
| `recommended_quantization` | Рекомендуемая квантизация (если не compatible) |
| `warning` | Предупреждение о нехватке памяти |

## Примечание

Проверка выполняется только для **локальных** моделей.
Облачные модели не требуют локальных ресурсов GPU.
"""

LOAD_MODEL = """
Явно загружает локальную модель в GPU VRAM.

## Зачем это нужно

По умолчанию локальные модели загружаются лениво (при первом использовании).
Этот endpoint позволяет:
- Предварительно загрузить модель до начала работы
- Контролировать время загрузки (не во время первого запроса)
- Получить точную информацию об использовании VRAM

## Как работает

1. Проверяет, зарегистрирована ли модель
2. Вызывает метод `load_model()` провайдера
3. Возвращает информацию о загрузке и VRAM

## Параметры

| Параметр | Тип | Описание |
|----------|-----|----------|
| `model_name` | string | Имя зарегистрированной модели |

## Ответ

| Поле | Описание |
|------|----------|
| `model_name` | Имя загруженной модели |
| `loaded` | True если модель загружена |
| `load_time_ms` | Время загрузки в миллисекундах |
| `vram_used_mb` | Использовано VRAM в MB |

## Примечание

Работает только для **локальных** моделей (GGUF через llama.cpp).
Облачные модели не требуют загрузки - они готовы к использованию сразу.
"""

UNLOAD_MODEL = """
Выгружает локальную модель из GPU VRAM для освобождения памяти.

## Зачем это нужно

- Освободить VRAM для загрузки другой модели
- Уменьшить использование памяти в idle периоды
- Ручное управление ресурсами GPU

## Как работает

1. Проверяет, зарегистрирована ли модель
2. Вызывает метод `unload_model()` провайдера
3. Освобождает VRAM
4. Модель остаётся зарегистрированной (можно загрузить снова)

## Параметры

| Параметр | Тип | Описание |
|----------|-----|----------|
| `model_name` | string | Имя зарегистрированной модели |

## Ответ

| Поле | Описание |
|------|----------|
| `model_name` | Имя выгруженной модели |
| `unloaded` | True если модель выгружена |
| `vram_freed_mb` | Освобождено VRAM в MB |

## Примечание

- Модель остаётся в registry и может быть загружена снова
- При следующем запросе генерации модель загрузится автоматически
- Работает только для локальных моделей
"""

DOWNLOAD_STATUS = """
Проверяет, скачана ли локальная модель и доступна ли она на HuggingFace Hub.

## Зачем это нужно

Перед регистрацией локальной модели полезно проверить:
- Есть ли модель уже на диске (не нужно качать заново)
- Доступна ли модель на HuggingFace для скачивания
- Какой размер файла модели

## Параметры

- **preset_name** (path) — имя локального пресета из `/models/presets`

## Что возвращает

| Поле | Описание |
|------|----------|
| `preset_name` | Имя пресета |
| `exists_locally` | Модель уже скачана |
| `local_path` | Путь к файлу на диске |
| `file_size_mb` | Размер файла в MB |
| `available_on_hf` | Доступна на HuggingFace |

## Использование

Если модель не скачана (`exists_locally=false`), она автоматически загрузится
при вызове `/models/register-from-preset` с `auto_download=true`.
"""
