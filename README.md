# SOP LLM Executor

**Ğ’Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ÑĞµÑ€Ğ²Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸**

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.115+-green.svg)](https://fastapi.tiangolo.com)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ

SOP LLM Executor â€” ÑÑ‚Ğ¾ production-ready Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ÑĞµÑ€Ğ²Ğ¸Ñ Ğ½Ğ° FastAPI Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ°Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½ "Dumb Executor" â€” ÑĞµÑ€Ğ²Ğ¸Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ inference, Ğ²ÑÑ Ğ±Ğ¸Ğ·Ğ½ĞµÑ-Ğ»Ğ¾Ğ³Ğ¸ĞºĞ° Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ¿ĞµÑ€ĞµĞ´Ğ°ÑÑ‚ÑÑ Ğ² Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ñ….

### ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸

- **ğŸ”Œ Ğ£Ğ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ** â€” Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Protocol
- **âš¡ ĞÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°** â€” Ğ¿Ğ¾Ğ»Ğ½Ğ°Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ° async/await, Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ
- **ğŸ¯ Single Worker** â€” GPU Guard Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞºÑĞºĞ»ÑĞ·Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº GPU
- **ğŸ“¦ Redis Storage** â€” Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ TTL 24 Ñ‡Ğ°ÑĞ°
- **ğŸ”„ Priority Queue** â€” Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ°Ğ¼Ğ¸
- **ğŸ” Idempotency** â€” Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ĞºĞ»ÑÑ‡Ğ¸ Ğ¸Ğ´ĞµĞ¼Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸
- **ğŸª Webhooks** â€” Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ»Ğ±ĞµĞºĞ¸ Ñ exponential backoff retry
- **ğŸ“Š Structured Output** â€” JSON Schema + GBNF Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°
- **ğŸ›ï¸ VRAM Monitoring** â€” Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ GPU Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸

## ĞŸĞ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ñ‹

| ĞŸÑ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€ | Ğ¢Ğ¸Ğ¿ | Streaming | Structured Output | VRAM Control |
|-----------|-----|-----------|-------------------|--------------|
| **Local (llama.cpp)** | GGUF | âœ… | âœ… (GBNF) | âœ… |
| **OpenAI** | API | âœ… | âœ… (JSON Schema) | â€” |
| **Anthropic** | API | âœ… | âœ… (JSON mode) | â€” |
| **OpenAI-Compatible** | API | âœ… | âš ï¸ | â€” |

## ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ HTTP POST /api/tasks
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         FastAPI Application             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   Routes   â”‚  â”‚   TaskProcessor  â”‚   â”‚
â”‚  â”‚  (tasks,   â”‚  â”‚   (Background    â”‚   â”‚
â”‚  â”‚  models,   â”‚  â”‚    Worker)       â”‚   â”‚
â”‚  â”‚  monitor)  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜            â”‚            â”‚
â”‚        â”‚                   â”‚            â”‚
â”‚        â–¼                   â–¼            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚       Session Store (Redis)      â”‚   â”‚
â”‚  â”‚  â€¢ Tasks (24h TTL)               â”‚   â”‚
â”‚  â”‚  â€¢ Priority Queue (Sorted Set)   â”‚   â”‚
â”‚  â”‚  â€¢ Idempotency Keys              â”‚   â”‚
â”‚  â”‚  â€¢ Logs                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GPU Guard  â”‚         â”‚  Provider   â”‚
â”‚  (Singleton)â”‚         â”‚  Registry   â”‚
â”‚             â”‚         â”‚             â”‚
â”‚  â€¢ Lock     â”‚         â”‚  â€¢ Local    â”‚
â”‚  â€¢ VRAM Mon â”‚         â”‚  â€¢ OpenAI   â”‚
â”‚             â”‚         â”‚  â€¢ Anthropicâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ°

### Ğ¢Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ

- **Python** 3.11+
- **Redis** 7.0+
- **NVIDIA GPU** (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾, Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹)
- **CUDA 12+** (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾)

### Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ ÑÑ‚Ğ°Ñ€Ñ‚

```bash
# ĞšĞ»Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¹
git clone git@github.com:VladPil/sop_llm.git
cd sop_llm

# Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ
python3.11 -m venv .venv
source .venv/bin/activate

# Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
pip install -e ".[dev]"

# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ (Ğ²Ñ‹Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ½ÑƒĞ¶Ğ½Ğ¾Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ)
cp .docker/configs/.env.local .env
# ĞÑ‚Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ .env

# Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ Redis
docker run -d -p 6379:6379 redis:7-alpine

# Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ ÑĞµÑ€Ğ²Ğ¸Ñ
python main.py
```

## ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ

Ğ’ÑĞµ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ `.env` Ñ„Ğ°Ğ¹Ğ»:

```env
# === Application ===
APP_NAME="SOP LLM Executor"
APP_ENV=production

# === Server ===
SERVER_HOST=0.0.0.0
SERVER_PORT=8000
SERVER_WORKERS=1  # Ğ’ĞĞ–ĞĞ: Single worker Ğ´Ğ»Ñ GPU Guard

# === Redis ===
REDIS_URL=redis://localhost:6379/0
REDIS_POOL_SIZE=10

# === GPU ===
GPU_INDEX=0
MAX_VRAM_USAGE_PERCENT=95
VRAM_RESERVE_MB=1024

# === Local Models ===
LOCAL_MODEL_PATH=/models/qwen2.5-7b-instruct.gguf
LOCAL_MODEL_CONTEXT_WINDOW=8192
LOCAL_MODEL_GPU_LAYERS=-1  # -1 = Ğ²ÑĞµ ÑĞ»Ğ¾Ğ¸ Ğ½Ğ° GPU

# === API Keys (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾) ===
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
```

## API Ğ”Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ

### ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ endpoints

#### Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸

```http
POST /api/tasks
Content-Type: application/json

{
  "model": "qwen-7b",
  "prompt": "ĞĞ°Ğ¿Ğ¸ÑˆĞ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¼Ğ°ÑÑĞ¸Ğ²Ğ°",
  "temperature": 0.7,
  "max_tokens": 1024,
  "stream": false,
  "webhook_url": "https://myapp.com/webhook",
  "idempotency_key": "user-123-req-456",
  "priority": 10.0
}
```

**Response:**
```json
{
  "task_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "pending",
  "model": "qwen-7b",
  "created_at": "2024-12-30T00:00:00Z",
  "updated_at": "2024-12-30T00:00:00Z"
}
```

#### ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ°Ñ‚ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸

```http
GET /api/tasks/{task_id}
```

**Response (completed):**
```json
{
  "task_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "completed",
  "model": "qwen-7b",
  "result": {
    "text": "def sort_array(arr):\\n    return sorted(arr)",
    "finish_reason": "stop",
    "usage": {
      "prompt_tokens": 15,
      "completion_tokens": 12,
      "total_tokens": 27
    }
  },
  "created_at": "2024-12-30T00:00:00Z",
  "updated_at": "2024-12-30T00:00:01Z",
  "finished_at": "2024-12-30T00:00:01Z"
}
```

#### Ğ ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸

```http
POST /api/models
Content-Type: application/json

{
  "name": "qwen-7b",
  "provider": "local",
  "config": {
    "model_path": "/models/qwen2.5-7b-instruct.gguf",
    "context_window": 8192,
    "gpu_layers": -1
  }
}
```

#### ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³

```http
GET /api/monitor/health
GET /api/monitor/gpu
GET /api/monitor/queue
```

### Swagger UI

ĞŸĞ¾Ğ»Ğ½Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ° Ğ¿Ğ¾ Ğ°Ğ´Ñ€ĞµÑÑƒ:
- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc
- **OpenAPI JSON**: http://localhost:8000/openapi.json

## ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ

### Python ĞºĞ»Ğ¸ĞµĞ½Ñ‚

```python
import httpx

async def generate_text():
    async with httpx.AsyncClient() as client:
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ
        response = await client.post(
            "http://localhost:8000/api/tasks",
            json={
                "model": "qwen-7b",
                "prompt": "ĞĞ±ÑŠÑÑĞ½Ğ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ¿ÑƒÑ‚Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ",
                "temperature": 0.7,
                "max_tokens": 500
            }
        )
        task = response.json()
        task_id = task["task_id"]

        # Ğ”Ğ¾Ğ¶Ğ´Ğ°Ñ‚ÑŒÑÑ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ
        while True:
            response = await client.get(
                f"http://localhost:8000/api/tasks/{task_id}"
            )
            task = response.json()
            if task["status"] in ["completed", "failed"]:
                break
            await asyncio.sleep(1)

        if task["status"] == "completed":
            print(task["result"]["text"])
```

### Ğ¡ webhook callback

```python
# Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ñ webhook
response = await client.post(
    "http://localhost:8000/api/tasks",
    json={
        "model": "qwen-7b",
        "prompt": "ĞĞ°Ğ¿Ğ¸ÑˆĞ¸ ÑÑ‚Ğ¸Ñ…Ğ¾Ñ‚Ğ²Ğ¾Ñ€ĞµĞ½Ğ¸Ğµ",
        "webhook_url": "https://myapp.com/llm-callback"
    }
)

# Ğ¡ĞµÑ€Ğ²Ğ¸Ñ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ POST Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ½Ğ° webhook_url Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ğ¸
```

### Ğ¡ JSON Schema (structured output)

```python
response = await client.post(
    "http://localhost:8000/api/tasks",
    json={
        "model": "gpt-4",
        "prompt": "Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ¸ Ğ¸Ğ¼Ñ, Ğ²Ğ¾Ğ·Ñ€Ğ°ÑÑ‚ Ğ¸ Ğ³Ğ¾Ñ€Ğ¾Ğ´ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°: 'ĞœĞµĞ½Ñ Ğ·Ğ¾Ğ²ÑƒÑ‚ Ğ˜Ğ²Ğ°Ğ½, Ğ¼Ğ½Ğµ 25 Ğ»ĞµÑ‚, Ğ¶Ğ¸Ğ²Ñƒ Ğ² ĞœĞ¾ÑĞºĞ²Ğµ'",
        "response_format": {
            "type": "object",
            "properties": {
                "name": {"type": "string"},
                "age": {"type": "number"},
                "city": {"type": "string"}
            },
            "required": ["name", "age", "city"]
        }
    }
)
```

## Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ

```bash
# Ğ’ÑĞµ Ñ‚ĞµÑÑ‚Ñ‹
pytest

# Unit Ñ‚ĞµÑÑ‚Ñ‹
pytest src/tests/unit -v

# Ğ¡ coverage
pytest --cov=src --cov-report=html

# Ğ›Ğ¸Ğ½Ñ‚Ğ¸Ğ½Ğ³
ruff check src

# Ğ¢Ğ¸Ğ¿Ñ‹
mypy src
```

## Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°

```
sop_llm/
â”œâ”€â”€ config/               # Pydantic Settings
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ settings.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/             # FastAPI routes
â”‚   â”‚   â”œâ”€â”€ routes/      # Endpoints (tasks, models, monitor)
â”‚   â”‚   â””â”€â”€ schemas/     # Pydantic models (requests, responses)
â”‚   â”œâ”€â”€ engine/          # GPU ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ
â”‚   â”‚   â”œâ”€â”€ gpu_guard.py       # Singleton GPU lock
â”‚   â”‚   â””â”€â”€ vram_monitor.py    # VRAM tracking
â”‚   â”œâ”€â”€ providers/       # LLM providers
â”‚   â”‚   â”œâ”€â”€ base.py            # Protocol + Pydantic models
â”‚   â”‚   â”œâ”€â”€ registry.py        # Provider registry
â”‚   â”‚   â”œâ”€â”€ local.py           # llama.cpp (GGUF)
â”‚   â”‚   â”œâ”€â”€ openai.py          # OpenAI API
â”‚   â”‚   â”œâ”€â”€ anthropic.py       # Anthropic API
â”‚   â”‚   â””â”€â”€ openai_compatible.py
â”‚   â”œâ”€â”€ services/        # Business logic
â”‚   â”‚   â”œâ”€â”€ session_store.py   # Redis storage
â”‚   â”‚   â””â”€â”€ task_processor.py  # Background worker
â”‚   â”œâ”€â”€ utils/           # Ğ£Ñ‚Ğ¸Ğ»Ğ¸Ñ‚Ñ‹
â”‚   â”‚   â””â”€â”€ logging.py
â”‚   â”œâ”€â”€ tests/           # Ğ¢ĞµÑÑ‚Ñ‹
â”‚   â”‚   â””â”€â”€ unit/
â”‚   â”œâ”€â”€ app.py           # FastAPI app
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ main.py              # Entry point
â”œâ”€â”€ pyproject.toml       # Ğ—Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
â”œâ”€â”€ README.md
â”œâ”€â”€ config/              # ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹ (pytest.ini, pre-commit)
â””â”€â”€ .docker/configs/     # Environment Ñ„Ğ°Ğ¹Ğ»Ñ‹ (.env.local, .env.dev, .env.prod)
```

## Production Deployment

### Docker

```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY . .

RUN pip install --no-cache-dir -e .

# Ğ’ĞĞ–ĞĞ: Single worker
CMD ["uvicorn", "src.app:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "1"]
```

### Docker Compose

```yaml
version: '3.8'

services:
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  app:
    build: .
    ports:
      - "8000:8000"
    environment:
      - REDIS_URL=redis://redis:6379/0
      - SERVER_WORKERS=1  # ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§ĞĞ
    depends_on:
      - redis
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
```

## Ğ›Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸Ñ

MIT License - ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¸Ñ‚Ğµ [LICENSE](LICENSE)

## ĞĞ²Ñ‚Ğ¾Ñ€

Vladislav <vladislav@example.com>
