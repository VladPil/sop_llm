# SOP LLM - GPU Override
# =============================================================================
# Используется вместе с docker-compose.local.yml для сборки с GPU/CUDA
#
# Использование:
#   docker compose -f docker-compose.local.yml -f docker-compose.gpu.yml build
#   docker compose -f docker-compose.local.yml -f docker-compose.gpu.yml up -d
#
# Или через Makefile:
#   make build-gpu
#   make up-gpu
# =============================================================================

services:
  sop_llm:
    build:
      target: runtime-cpu  # GPU не нужен для API сервиса - Ollama обрабатывает GPU
    image: sop_llm:local-gpu
    environment:
      # Ollama endpoint для LiteLLM
      - OLLAMA_API_BASE=http://ollama:11434
    depends_on:
      - ollama

  # Ollama - локальный LLM сервер с GPU
  ollama:
    image: ollama/ollama:latest
    container_name: sop_llm_ollama
    ports:
      - "11434:11434"
    volumes:
      # Кэш моделей Ollama
      - ollama_models:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    networks:
      - sop_network
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped

volumes:
  ollama_models:
    name: sop_llm_ollama_models
