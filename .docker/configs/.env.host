# =============================================================================
# SOP LLM Service - Host Development Environment
# =============================================================================
# Backend запускается на хосте (использует VPN хоста для внешних API)
# Инфраструктура (Redis, Langfuse) в Docker
# =============================================================================

# =============================================================================
# Application Settings
# =============================================================================
APP_NAME="SOP LLM Service"
APP_ENV=local
DEBUG=true
LOG_LEVEL=DEBUG
LOG_FORMAT=json

# =============================================================================
# Server Settings
# =============================================================================
SERVER_HOST=0.0.0.0
SERVER_PORT=8000

# =============================================================================
# Redis Connection (из sop_infrastructure на localhost:6380)
# =============================================================================
REDIS_HOST=localhost
REDIS_PORT=6380
REDIS_DB=0
REDIS_PASSWORD=change_me_in_production
REDIS_URL=redis://:change_me_in_production@localhost:6380/0

# Redis Session Settings
SESSION_TTL_SECONDS=3600
IDEMPOTENCY_TTL_SECONDS=86400
LOGS_MAX_RECENT=100

# =============================================================================
# LLM Settings
# =============================================================================
MODELS_DIR=./models
DEFAULT_CONTEXT_WINDOW=4096
DEFAULT_MAX_TOKENS=2048

# =============================================================================
# Langfuse Observability (из sop_infrastructure на localhost:3001)
# =============================================================================
# UI: http://localhost:3001 (admin@local.dev / admin123)
LANGFUSE_ENABLED=true
LANGFUSE_HOST=http://localhost:3001
LANGFUSE_PUBLIC_KEY=pk-lf-local-dev-public-key
LANGFUSE_SECRET_KEY=sk-lf-local-dev-secret-key

# =============================================================================
# LiteLLM Configuration
# =============================================================================
LITELLM_DEBUG=false
LITELLM_DROP_PARAMS=true
LITELLM_MAX_RETRIES=3
LITELLM_TIMEOUT=600

# =============================================================================
# Ollama Configuration (если Ollama на хосте)
# =============================================================================
OLLAMA_API_BASE=http://localhost:11434

# =============================================================================
# External LLM Provider API Keys
# =============================================================================
# Эти запросы пойдут через VPN хоста!

# OpenAI (GPT models)
OPENAI_API_KEY=${OPENAI_API_KEY:-}
OPENAI_BASE_URL=${OPENAI_BASE_URL:-}

# Anthropic (Claude models)
ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}

# Google Gemini
GEMINI_API_KEY=${GEMINI_API_KEY:-}

# Mistral AI
MISTRAL_API_KEY=${MISTRAL_API_KEY:-}

# HuggingFace
HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN:-}

# =============================================================================
# GPU Settings
# =============================================================================
DEVICE=cpu
GPU_INDEX=0
MAX_VRAM_USAGE_PERCENT=90.0
VRAM_RESERVE_MB=512

# =============================================================================
# HTTP Settings
# =============================================================================
WEBHOOK_TIMEOUT_SECONDS=30
WEBHOOK_MAX_RETRIES=3
HTTP_TIMEOUT_SECONDS=60
HTTP_MAX_RETRIES=2

# =============================================================================
# CORS Settings
# =============================================================================
CORS_ALLOWED_ORIGINS='["*"]'

# =============================================================================
# Performance Settings
# =============================================================================
WORKERS=2
THREADS=8
