# =============================================================================
# SOP LLM Service - Local Development Environment
# =============================================================================
# Для локальной разработки с подключением к централизованной инфраструктуре
# Инфраструктура (Redis, Langfuse, etc.) запущена в sop_infrastructure
# =============================================================================

# =============================================================================
# Application Settings
# =============================================================================
APP_NAME=SOP LLM Service
APP_ENV=local
DEBUG=true
LOG_LEVEL=DEBUG
LOG_FORMAT=json

# =============================================================================
# Server Settings
# =============================================================================
SERVER_HOST=0.0.0.0
SERVER_PORT=8000

# =============================================================================
# Service Ports (согласно PORT_MAPPING.md)
# =============================================================================
# sop_llm: блок 8200-8299
APP_PORT=8200      # Внешний порт API (localhost:8200)

# =============================================================================
# Redis Connection (Централизованная инфраструктура)
# =============================================================================
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_DB=1  # Уникальная БД для sop_llm (0 для intake, 1 для llm, 2 для s3)
REDIS_PASSWORD=change_me_in_production
REDIS_URL=redis://:change_me_in_production@redis:6379/1

# =============================================================================
# Kafka Connection (Централизованная инфраструктура)
# =============================================================================
KAFKA_BOOTSTRAP_SERVERS=kafka:9092

# Redis Session Settings
SESSION_TTL_SECONDS=3600
IDEMPOTENCY_TTL_SECONDS=86400
LOGS_MAX_RECENT=100

# =============================================================================
# LLM Settings
# =============================================================================
MODELS_DIR=/app/models
DEFAULT_CONTEXT_WINDOW=4096
DEFAULT_MAX_TOKENS=2048

# =============================================================================
# Langfuse Observability (Self-hosted)
# =============================================================================
# UI: http://localhost:3001 (admin@local.dev / admin123)
LANGFUSE_ENABLED=true
LANGFUSE_HOST=http://langfuse:3000
LANGFUSE_PORT=3001

# Langfuse Server Configuration
LANGFUSE_DATABASE_URL=postgresql://sop_admin:change_me_in_production@postgres:5432/langfuse
LANGFUSE_NEXTAUTH_SECRET=langfuse-secret-key-change-in-production
LANGFUSE_NEXTAUTH_URL=http://localhost:3001
LANGFUSE_SALT=langfuse-salt-change-in-production
LANGFUSE_TELEMETRY_ENABLED=false
LANGFUSE_AUTH_DISABLE_SIGNUP=false
LANGFUSE_ENABLE_EXPERIMENTAL_FEATURES=true

# Langfuse Auto-initialization (создаёт пользователя, проект, ключи)
LANGFUSE_INIT_ORG_ID=sop-local-org
LANGFUSE_INIT_ORG_NAME=SOP Local Development
LANGFUSE_INIT_PROJECT_ID=sop-llm-local
LANGFUSE_INIT_PROJECT_NAME=SOP LLM Service
LANGFUSE_INIT_PROJECT_PUBLIC_KEY=pk-lf-local-dev-public-key
LANGFUSE_INIT_PROJECT_SECRET_KEY=sk-lf-local-dev-secret-key
LANGFUSE_INIT_USER_EMAIL=admin@local.dev
LANGFUSE_INIT_USER_NAME=Local Admin
LANGFUSE_INIT_USER_PASSWORD=admin123

# Ключи для SDK (берутся из auto-init)
LANGFUSE_PUBLIC_KEY=pk-lf-local-dev-public-key
LANGFUSE_SECRET_KEY=sk-lf-local-dev-secret-key

# =============================================================================
# LiteLLM Configuration
# =============================================================================
LITELLM_DEBUG=false
LITELLM_DROP_PARAMS=true
LITELLM_MAX_RETRIES=3
LITELLM_TIMEOUT=600

# =============================================================================
# Ollama Configuration (локальные модели через GPU)
# =============================================================================
# Ollama endpoint для LiteLLM (ollama/model_name)
OLLAMA_API_BASE=http://ollama:11434

# =============================================================================
# External LLM Provider API Keys
# =============================================================================
# OpenAI (GPT models)
OPENAI_API_KEY=${OPENAI_API_KEY:-}
OPENAI_BASE_URL=${OPENAI_BASE_URL:-}

# Anthropic (Claude models)
ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}

# OpenAI-Compatible (Ollama, LocalAI, etc.)
OPENAI_COMPATIBLE_BASE_URL=${OPENAI_COMPATIBLE_BASE_URL:-}
OPENAI_COMPATIBLE_API_KEY=${OPENAI_COMPATIBLE_API_KEY:-}

# Google Gemini
GEMINI_API_KEY=${GEMINI_API_KEY:-}

# Mistral AI
MISTRAL_API_KEY=${MISTRAL_API_KEY:-}

# HuggingFace (для загрузки моделей)
HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN:-}

# =============================================================================
# GPU Settings (Local Models)
# =============================================================================
# Для локальной разработки GPU обычно не используется
INSTALL_GPU=false
CUDA_VERSION=12.1.0
DEVICE=cpu

GPU_INDEX=0
MAX_VRAM_USAGE_PERCENT=90.0
VRAM_RESERVE_MB=512

# =============================================================================
# HTTP Settings
# =============================================================================
WEBHOOK_TIMEOUT_SECONDS=30
WEBHOOK_MAX_RETRIES=3
HTTP_TIMEOUT_SECONDS=60
HTTP_MAX_RETRIES=2

# =============================================================================
# CORS Settings
# =============================================================================
CORS_ALLOWED_ORIGINS=["*"]

# =============================================================================
# Performance Settings
# =============================================================================
WORKERS=2
THREADS=8
TORCH_NUM_THREADS=8
OMP_NUM_THREADS=8

# =============================================================================
# Feature Flags
# =============================================================================
ENABLE_JSON_FIXING=false
JSON_FIXER_TIMEOUT=30
